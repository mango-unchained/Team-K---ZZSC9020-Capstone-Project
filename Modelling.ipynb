{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3d38702-d15a-46fc-807f-1ebfa3cd13e8",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a53e1b5-e3d8-4df6-92de-6b1d97c37f08",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from re import match\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sys import stdout\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "from json import dumps, loads\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Bidirectional\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import (\n",
    "    MAPE, MeanAbsolutePercentageError, MSE\n",
    ")\n",
    "from tensorflow.keras.callbacks import (\n",
    "    Callback, LearningRateScheduler, ModelCheckpoint\n",
    ")\n",
    "from tensorflow.keras.regularizers import L1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09973142-da18-4b0f-a523-617d9c6b4e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler1(epoch, lr): \n",
    "    \"\"\"\n",
    "Learning rate scheduler that alternates between higher and lower rates.\\\n",
    "\"\"\"\n",
    "    return 0.0001 if epoch % 2 == 1 else 0.001\n",
    "    \n",
    "def scheduler2(epoch, lr):\n",
    "    \"\"\"\\\n",
    "Learning rate scheduler that alternates between higher and \\\n",
    "lower rates for half of the training and uses the lower rate afterwards.\\\n",
    "\"\"\"\n",
    "    return 0.0001 if ((epoch % 2 == 1) or (epoch > epochs/2)) else 0.001\n",
    "    \n",
    "def scheduler3(epoch, lr):\n",
    "    \"\"\"Learning rate scheduler that only uses the lower rate.\"\"\"\n",
    "    return 0.0001\n",
    "\n",
    "def scheduler4(epoch, lr):\n",
    "    \"\"\"\\\n",
    "Learning rate scheduler that alternates between higher and \\\n",
    "lower rates until the final 100 epochs, when only the lower rate is used.\\\n",
    "\"\"\"\n",
    "    return 0.0001 if ((epoch % 2 == 1) or (epoch > epochs-100)) else 0.001\n",
    "\n",
    "class ProgressBar(Callback):\n",
    "    \"\"\"\\\n",
    "A progress bar displays while fitting models \\\n",
    "showing the relative progess and estimated time to competion.\\\n",
    "\"\"\"\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epochs = self.__dict__['params']['epochs']\n",
    "        self.steps = self.__dict__['params']['steps'] \n",
    "        # Calculate the number of steps to completion.\n",
    "        self.total_steps = self.steps * self.epochs \n",
    "        # Generate progress bar.\n",
    "        self.progress_bar = tqdm(\n",
    "            desc='Training', total=self.total_steps, unit=' step', \n",
    "            smoothing=0, file=stdout, colour='green', position=0, \n",
    "            ncols=100, unit_scale=1/self.steps,\n",
    "            bar_format='{l_bar}{bar}| {n:.0f}/{total:.0f} epochs \\\n",
    "completed [{elapsed}<{remaining}, {rate_fmt}]',\n",
    "        )\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        # Update the progress bar by one step after every batch.\n",
    "        self.progress_bar.update(1)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        # Close the bar when training is finished.\n",
    "        self.progress_bar.close()\n",
    "\n",
    "class SaveTrainTime(Callback):\n",
    "    '''\\\n",
    "Record the training session metadata when a model completes fitting.\\\n",
    "'''\n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time = datetime.now()\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        # Load and format the CSV of training times.\n",
    "        ttimes = pd.read_csv('models/train_times.csv')\n",
    "        ttimes.train_time = ttimes.train_time.astype('timedelta64[ns]')\n",
    "        params = self.__dict__['params']\n",
    "        # Record the current session and save the results.\n",
    "        ttimes = pd.concat([\n",
    "            ttimes,\n",
    "            pd.DataFrame([[\n",
    "                self.model.name, \n",
    "                self.start_time, \n",
    "                datetime.now() - self.start_time, \n",
    "                params['epochs'], \n",
    "                params['steps'],\n",
    "            ]], columns=ttimes.columns) \n",
    "        ])\n",
    "        \n",
    "        ttimes.to_csv('models/train_times.csv', index=False)\n",
    "\n",
    "def get_MLP(X_train, n_hidden=1, univariate=False, model_prefix=''):\n",
    "    \"\"\"Build a multilayer perception. Return the uncompiled model.\n",
    "\n",
    "    Args:\n",
    "        X_train (tf.Tensor): Input training data.\n",
    "        n_hidden (int): Number of hidden layer in neural network.\n",
    "        univariate (Bool): Whether or not the model to be \\\n",
    "produced is univariate.\n",
    "        model_prefix (string): Text to be prepended to the model name.\n",
    "\n",
    "    Returns:\n",
    "        model_kwargs (dict): A dictionary including the uncompiled model\\\n",
    " to pass to train_model() as kwargs.\n",
    "    \"\"\"\n",
    "    tf.keras.utils.set_random_seed(1)\n",
    "\n",
    "    # Define the input layer.\n",
    "    main_input = Input(shape=tuple(X_train[0].shape), name=\"input\")\n",
    "    previous_layer = main_input\n",
    "\n",
    "    # Add as many hidden layers as are specified, \n",
    "    # alternating between linear and relu activation functions.\n",
    "    for i in range(n_hidden):\n",
    "        if i % 2 == 1:\n",
    "            previous_layer = Dense(\n",
    "                32, name=f\"linear_{i+1}\", activation='linear'\n",
    "            )(previous_layer)\n",
    "        else:\n",
    "            previous_layer = Dense(\n",
    "                32, name=f\"relu_{i+1}\", activation='relu'\n",
    "            )(previous_layer)\n",
    "\n",
    "    # Create the linear output layer.\n",
    "    main_output = Dense(2, name=f\"output\")(previous_layer)\n",
    "\n",
    "    # Name the model.\n",
    "    model_prefix = model_prefix + '_' if model_prefix else model_prefix\n",
    "    name = f\"\"\"{model_prefix}MLP_{\n",
    "        'shallow' if n_hidden <= 1 else 'deep'\n",
    "    }_{\n",
    "        'univ' if univariate else 'multiv'\n",
    "    }\"\"\"\n",
    "\n",
    "    # Return model and additional information \n",
    "    # for the train_model() function.\n",
    "    return {\n",
    "        'model': Model(\n",
    "            inputs=main_input, outputs=main_output, name=name\n",
    "        ),\n",
    "        'univariate': univariate,\n",
    "        'X_train': X_train,\n",
    "    }\n",
    "\n",
    "def get_LSTM(X_train, n_hidden=1, univariate=False, model_prefix=''):\n",
    "    \"\"\"Build a bidirectional LSTM. Return the uncompiled model.\n",
    "\n",
    "    Args:\n",
    "        X_train (tf.Tensor): Input training data.\n",
    "        n_hidden (int): Number of hidden layer in neural network.\n",
    "        univariate (Bool): Whether or not the model will be univariate.\n",
    "        model_prefix (string): Text to be prepended to the model name.\n",
    "\n",
    "    Returns:\n",
    "        model_kwargs (dict): A dictionary including the uncompiled model\\\n",
    " to pass to train_model() as kwargs.\n",
    "    \"\"\"\n",
    "    tf.keras.utils.set_random_seed(1)\n",
    "    \n",
    "    # Define the input layer.\n",
    "    main_input = Input(shape=(X_train[0].shape[0],1), name=\"input\")\n",
    "    previous_layer = main_input\n",
    "    # Add as many hidden layers as are specified, starting with the \n",
    "    # widest. The width is defined by powers of 2.\n",
    "    for i in range(n_hidden-1):\n",
    "        previous_layer = Bidirectional(\n",
    "            LSTM(2**(4+n_hidden-i), return_sequences=True), \n",
    "            name=f\"BD_{i+1}\"\n",
    "        )(previous_layer)\n",
    "    previous_layer = Bidirectional(\n",
    "        LSTM(2**5), name=f\"BD_{n_hidden}\"\n",
    "    )(previous_layer)\n",
    "\n",
    "    # Create the linear output layer.\n",
    "    main_output = Dense(2, name=f\"output\")(previous_layer)\n",
    "\n",
    "    # Name the model.\n",
    "    model_prefix = model_prefix + '_' if model_prefix else model_prefix\n",
    "    name = f\"\"\"{model_prefix}LSTM_{\n",
    "        'shallow' if n_hidden <= 1 else 'deep'\n",
    "    }_{\n",
    "        'univ' if univariate else 'multiv'\n",
    "    }\"\"\"\n",
    "    \n",
    "    # Return model and other information for the train_model() function.\n",
    "    return {\n",
    "        'X_train': X_train,\n",
    "        'model': Model(\n",
    "            inputs=main_input, outputs=main_output, name=name\n",
    "        ), 'univariate': univariate,\n",
    "    }\n",
    "\n",
    "def _compile_model(model, opt_params={}):\n",
    "    \"\"\"Compile an uncompiled Tensorflow model. Return the compiled model.\n",
    "\n",
    "    Args:\n",
    "        model (keras.src.models.functional.Functional): \\\n",
    "The uncompiled model.\n",
    "        opt_params (dict): A kwargs dictionary used with the optimiser.\n",
    "\n",
    "    Returns:\n",
    "        model (keras.src.models.functional.Functional): \\\n",
    "The compiled model.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(**opt_params), \n",
    "        loss=MeanAbsolutePercentageError(), \n",
    "        metrics=[MSE]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Define the optimiser hyperparameters of the BD-LSTMs. This is declared \n",
    "# globally, as it is used in both train_model() and load_lstm().\n",
    "lstm_opt_params = {'weight_decay': 1e-05, 'beta_1': 0.8}\n",
    "\n",
    "def train_model(\n",
    "    y_train, X_val, y_val, X_train, model, \n",
    "    univariate=False, epochs=2000, opt_params={}, fit_params={}\n",
    "):\n",
    "    \"\"\"Train an uncompiled Tensorflow model. \\\n",
    "Return the trained model and its training history.\n",
    "\n",
    "    Args:\n",
    "        y_train (tf.Tensor): Target training data.\n",
    "        X_val (tf.Tensor): Input validation data.\n",
    "        y_val (tf.Tensor): Target validation data.\n",
    "        X_train (tf.Tensor): Input training data.\n",
    "        model (keras.src.models.functional.Functional): \\\n",
    "The uncompiled model.\n",
    "        univariate (bool): True if model is univariate and False \\\n",
    "if multivariate.\n",
    "        epochs (int): Number of training epochs.\n",
    "        opt_params (dict): A kwargs dictionary used with the model's \\\n",
    "optimiser.\n",
    "        fit_params (dict): A kwargs dictionary used with the model's \\\n",
    "fit() method.\n",
    "\n",
    "    Returns:\n",
    "        model (keras.src.models.functional.Functional): The trained model.\n",
    "        history (pd.DataFrame): The record of training and validation \\\n",
    "loss and metric values at successive epochs.\n",
    "    \"\"\"\n",
    "    # Define the hyperparameters and checkpoint callback for the model. \n",
    "    if 'MLP' in model.name:\n",
    "        opt_params = opt_params if opt_params \\\n",
    "            else {'weight_decay': 1e-05, 'beta_1': 0.95}\n",
    "        fit_params = fit_params if fit_params \\\n",
    "            else {'batch_size': 2**8}\n",
    "        # The model checkpoint will save the best model, \n",
    "        # based on validation loss, from the fitting process.\n",
    "        model_checkpoints = ModelCheckpoint(\n",
    "            f\"models/{model.name}.keras\", save_best_only=True\n",
    "        )\n",
    "    elif 'LSTM' in model.name:\n",
    "        opt_params = opt_params if opt_params else lstm_opt_params\n",
    "        fit_params = fit_params if fit_params else {'batch_size': 2**8}\n",
    "        # The model checkpoint will save the best model, \n",
    "        # based on validation loss, during the fitting process.\n",
    "        # A bug in Tensorflow has meant that only the weights of \n",
    "        # BD-LSTMs can be loaded, not the whole model in a KERAS file. \n",
    "        model_checkpoints = ModelCheckpoint(\n",
    "            f\"models/{model.name}.weights.h5\", \n",
    "            save_best_only=True, save_weights_only=True\n",
    "        )\n",
    "\n",
    "    model = _compile_model(model, opt_params=opt_params)\n",
    "\n",
    "    # Fit the model on the training day \n",
    "    # while testing, after each epoch, on the validation data.\n",
    "    history = model.fit(\n",
    "        X_train, y_train, verbose=0, shuffle=False, epochs=epochs,\n",
    "            validation_data=(X_val, y_val), callbacks=[\n",
    "            model_checkpoints, # Save the model.\n",
    "            # Alternate the learning rate.\n",
    "            LearningRateScheduler(scheduler1), \n",
    "            ProgressBar(), # Show the progress bar.\n",
    "            SaveTrainTime(), # Save the fitting metadata.\n",
    "        ], **fit_params\n",
    "    )\n",
    "    # Record the training history locally.\n",
    "    history = pd.DataFrame(history.history)\n",
    "    history.to_csv(\n",
    "        f'models/{model.name} history.csv', \n",
    "        index=False, lineterminator='\\n'\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def load_lstm(name):\n",
    "    \"\"\"Take the name of a model and return the most trained model \\\n",
    "stored locally. Note that this method will become redundant once a \\\n",
    "bug in Tensorflow stopping KERAS files of these models from being \\\n",
    "loaded is fixed.\"\"\"\n",
    "    # Select the right data for this model.\n",
    "    if 'univ' in name:\n",
    "        _X_train, _y_train, _X_test, _y_test = \\\n",
    "            X_train_uv, y_train_full, X_test_uv, y_test\n",
    "        univariate = True\n",
    "    else:\n",
    "        _X_train, _y_train, _X_test, _y_test = \\\n",
    "            X_train_full, y_train_full, X_test, y_test\n",
    "        univariate = False\n",
    "    # Define the depth of the architecture.\n",
    "    if 'deep' in name:\n",
    "        n_hidden = 2\n",
    "    else:\n",
    "        n_hidden = 1\n",
    "\n",
    "    # Get the prefix of the most trained model stored locally.\n",
    "    prefix = name.split('_LSTM')[0]\n",
    "    # Build the model.\n",
    "    lstm_data = get_LSTM(\n",
    "        _X_train, n_hidden, univariate=univariate, model_prefix=prefix\n",
    "    )\n",
    "    model = _compile_model(lstm_data['model'])\n",
    "\n",
    "    # Load the saved weights.\n",
    "    model.load_weights(f\"models/{name}.weights.h5\")\n",
    "\n",
    "    # Re-apply the hyperparameters used in the saved model.\n",
    "    for k, v in lstm_opt_params.items():\n",
    "        exec(f'model.optimizer.{k} = {v}')\n",
    "\n",
    "    return model\n",
    "\n",
    "def flatten_params(param_grid):\n",
    "    \"\"\"Take a paramater grid and return a flattened list of each \\\n",
    "possible combination of the parameters.\"\"\"\n",
    "    return [\n",
    "        dict(zip(param_grid.keys(), e)) \n",
    "        for e in product(*param_grid.values())\n",
    "    ]\n",
    "\n",
    "def GridSearchCV(estimator, param_grid, epochs):\n",
    "    \"\"\"Take a callable model generator and a parameter grid and \\\n",
    "return the best possible set of parameters given the specified epochs.\n",
    "\n",
    "    Args:\n",
    "        estimator (function): A function returning an uncompiled model.\n",
    "        param_grid (dict): A dictionary of lists of hyperparameters \\\n",
    "specifiying the hyperparameter space to be searched.\n",
    "        epochs (int): Number of training epochs.\n",
    "\n",
    "    Returns:\n",
    "        dict: The hyperparameters used to train the best model.\n",
    "    \"\"\"\n",
    "    est_params = {} # Parameters for building the model.\n",
    "    opt_params = {} # Parameters of the optimiser.\n",
    "    params = {} # Parameters of Model.fit method.\n",
    "    # Split the parameter grid into separate dictionaries.\n",
    "    for k, v in param_grid.items():\n",
    "        if (est_lab := 'estimator__') in k:\n",
    "            est_params.update({k.replace(est_lab, ''): v})\n",
    "        elif (opt_lab := 'optimizer__') in k:\n",
    "            opt_params.update({k.replace(opt_lab, ''): v})\n",
    "        else:\n",
    "            params.update({k: v})\n",
    "\n",
    "    # Expand the grid into each combination to be tested.\n",
    "    param_grid_list = flatten_params({\n",
    "        'est_params': flatten_params(est_params), \n",
    "        'opt_params': flatten_params(opt_params), \n",
    "        'params': flatten_params(params),\n",
    "    })\n",
    "    n_iter = len(param_grid_list)\n",
    "    results = defaultdict(list)\n",
    "\n",
    "    # Test each combination of parameters. \n",
    "    # Represent the process with a progress bar.\n",
    "    for i in tqdm(\n",
    "        range(n_iter), desc='Searching hyperparameter space', \n",
    "        file=stdout, colour='green'\n",
    "    ):\n",
    "        param_grid = param_grid_list[i]\n",
    "        # For each train/validation set...\n",
    "        for j in range(len(X_train_sets)):\n",
    "            # Load the train/validation data.\n",
    "            X_train, y_train, X_val, y_val = \\\n",
    "                X_train_sets[j], y_train_sets[j], \\\n",
    "                X_val_sets[j], y_val_sets[j]\n",
    "\n",
    "            # Get the current test parameters and run the model.\n",
    "            est_params = param_grid['est_params']\n",
    "            opt_params = param_grid['opt_params']\n",
    "            params = param_grid['params']\n",
    "\n",
    "            model, _, _ = estimator(X_train, **est_params).values()\n",
    "            \n",
    "            model.compile(\n",
    "                optimizer=Adam(**opt_params), \n",
    "                loss=MeanAbsolutePercentageError(), \n",
    "            )\n",
    "            history = model.fit(\n",
    "                X_train, y_train, verbose=0, shuffle=False, \n",
    "                epochs=epochs, validation_data=(X_val, y_val), \n",
    "                callbacks=[LearningRateScheduler(scheduler1)], \n",
    "                **params\n",
    "            )\n",
    "            # Save the results.\n",
    "            results[i].append(model.evaluate(X_val, y_val, verbose=0))\n",
    "\n",
    "    # Return the set of parameters that have \n",
    "    # the lowest mean validation loss.\n",
    "    return param_grid_list[pd.Series(results).apply(np.mean).argmin()]\n",
    "\n",
    "def extend_training(model):\n",
    "    '''Rename a previous model so that it can be further trained \\\n",
    "without overwriting any records.'''\n",
    "    model.name = f\"Extended_{model.name}\"\n",
    "    return model\n",
    "\n",
    "def get_latest_models(name=None):\n",
    "    '''Return information on the most trained iterations of over model. \\\n",
    "If a specific model is named, only return the name of the most trained \\\n",
    "iteration.'''\n",
    "    # List each model stored locally.\n",
    "    all_models = pd.DataFrame([\n",
    "        e.replace('\\\\', '/') for e in glob('models/*') \n",
    "        if match(r'.*\\.(?:keras|h5)$', e)\n",
    "    ], columns=['file'])\n",
    "    # Extract the model name, the iteration name, the count of training \n",
    "    # sessions, and whether it is the latest iteration.\n",
    "    all_models['model'] = all_models.file.str\\\n",
    "        .extract(r'models/.*Final_(.*?)\\..*$')\n",
    "    all_models['model_name'] = all_models.file.str\\\n",
    "        .extract(r'models/(.*Final_.*?)\\..*$')\n",
    "    all_models['extensions'] = all_models.file\\\n",
    "        .apply(lambda e: e.count('Extended'))\n",
    "    all_models['latest'] = all_models.groupby('model')\\\n",
    "        .extensions.transform('max')\n",
    "    # Filter out older iterations.\n",
    "    latest = all_models.loc[\n",
    "        all_models.extensions == all_models.latest, \n",
    "        ['model', 'model_name', 'file']\n",
    "    ]\n",
    "    \n",
    "    if name:\n",
    "        latest = latest.loc[latest.model == name, 'model_name']\\\n",
    "            .squeeze()\n",
    "    \n",
    "    return latest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f509dbaa-fbd2-4d16-8abe-280e0f31ffd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3173190-fea4-4880-aba0-d9d721cdb111",
   "metadata": {},
   "source": [
    "# Data preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9d6f9ab-2e70-40a8-ba86-8d933194960f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(104758,) (104755,)\n",
      "(209513,) (104755,)\n",
      "(314268,) (104755,)\n",
      "(419023,) (104755,)\n",
      "(523778,) (104755,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(104758, 70), dtype=float64, numpy=\n",
       " array([[ 0.9132249 ,  0.9131219 , -1.58047303, ...,  0.87825308,\n",
       "          1.00316397,  1.09089092],\n",
       "        [ 0.09708733,  0.86111884, -1.58047303, ...,  0.04669082,\n",
       "          0.0874565 ,  0.1407623 ],\n",
       "        [-0.15027044,  0.01142544, -1.58047303, ..., -0.08985632,\n",
       "         -0.03193541,  0.08894003],\n",
       "        ...,\n",
       "        [-1.47230972,  0.30641948, -1.27130153, ..., -1.38906997,\n",
       "         -1.3868722 , -1.36763819],\n",
       "        [ 1.74346793, -0.57763261, -1.27130153, ...,  1.67796777,\n",
       "          1.66124456,  1.67802582],\n",
       "        [ 0.4047031 , -0.02989103, -1.27130153, ...,  0.4411795 ,\n",
       "          0.45533762,  0.47632769]])>,\n",
       " <tf.Tensor: shape=(209513, 70), dtype=float64, numpy=\n",
       " array([[ 0.9132249 ,  0.9131219 , -1.58047303, ...,  0.87825308,\n",
       "          1.00316397,  1.09089092],\n",
       "        [ 0.09708733,  0.86111884, -1.58047303, ...,  0.04669082,\n",
       "          0.0874565 ,  0.1407623 ],\n",
       "        [-0.15027044,  0.01142544, -1.58047303, ..., -0.08985632,\n",
       "         -0.03193541,  0.08894003],\n",
       "        ...,\n",
       "        [ 0.17438927, -0.75097616, -0.96213002, ...,  0.23096711,\n",
       "          0.14301518,  0.11878895],\n",
       "        [-0.1524954 ,  0.41042561, -0.96213002, ..., -0.19002924,\n",
       "         -0.2222075 , -0.24990141],\n",
       "        [ 0.19636265, -0.85498229, -0.96213002, ...,  0.33165024,\n",
       "          0.23096616,  0.14301411]])>,\n",
       " <tf.Tensor: shape=(314268, 70), dtype=float64, numpy=\n",
       " array([[ 0.9132249 ,  0.9131219 , -1.58047303, ...,  0.87825308,\n",
       "          1.00316397,  1.09089092],\n",
       "        [ 0.09708733,  0.86111884, -1.58047303, ...,  0.04669082,\n",
       "          0.0874565 ,  0.1407623 ],\n",
       "        [-0.15027044,  0.01142544, -1.58047303, ..., -0.08985632,\n",
       "         -0.03193541,  0.08894003],\n",
       "        ...,\n",
       "        [-1.28280852, -0.889651  , -0.34378702, ..., -1.33038098,\n",
       "         -1.30991941, -1.29332469],\n",
       "        [ 1.89428555, -1.0803289 , -0.34378702, ...,  1.82109387,\n",
       "          1.89826759,  1.99085448],\n",
       "        [-1.29791906, -0.82031358, -0.34378702, ..., -1.34557974,\n",
       "         -1.33038134, -1.30991979]])>,\n",
       " <tf.Tensor: shape=(419023, 70), dtype=float64, numpy=\n",
       " array([[ 0.9132249 ,  0.9131219 , -1.58047303, ...,  0.87825308,\n",
       "          1.00316397,  1.09089092],\n",
       "        [ 0.09708733,  0.86111884, -1.58047303, ...,  0.04669082,\n",
       "          0.0874565 ,  0.1407623 ],\n",
       "        [-0.15027044,  0.01142544, -1.58047303, ..., -0.08985632,\n",
       "         -0.03193541,  0.08894003],\n",
       "        ...,\n",
       "        [ 1.04049632,  0.79178142, -0.03461552, ...,  1.17141799,\n",
       "          1.17704044,  1.17312218],\n",
       "        [-1.40859153,  1.15580287, -0.03461552, ..., -1.47352243,\n",
       "         -1.48232665, -1.49319853],\n",
       "        [ 0.58667397,  1.15580287, -0.03461552, ...,  0.58396034,\n",
       "          0.59626556,  0.6064185 ]])>,\n",
       " <tf.Tensor: shape=(523778, 70), dtype=float64, numpy=\n",
       " array([[ 0.9132249 ,  0.9131219 , -1.58047303, ...,  0.87825308,\n",
       "          1.00316397,  1.09089092],\n",
       "        [ 0.09708733,  0.86111884, -1.58047303, ...,  0.04669082,\n",
       "          0.0874565 ,  0.1407623 ],\n",
       "        [-0.15027044,  0.01142544, -1.58047303, ..., -0.08985632,\n",
       "         -0.03193541,  0.08894003],\n",
       "        ...,\n",
       "        [-1.32411602, -0.66430439,  0.58372748, ..., -1.39276033,\n",
       "         -1.36068293, -1.31429681],\n",
       "        [ 1.32660289, -1.54835649,  0.58372748, ...,  1.16565996,\n",
       "          1.2267989 ,  1.30508886],\n",
       "        [ 0.1897223 , -0.66430439,  0.58372748, ...,  0.01551073,\n",
       "          0.08551158,  0.13516539]])>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and prepare the modelling data.\n",
    "df = pd.read_csv(\n",
    "    'data/modelling_data.csv', parse_dates=['DATETIME'], \n",
    "    date_format='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "df = df.set_index('DATETIME').dropna().sort_index()\n",
    "\n",
    "# Split the dataframe into X and y data.\n",
    "y_cols = ['h1_TOTALDEMAND', 'h24_TOTALDEMAND']\n",
    "X_df, y_df = df.drop(columns=y_cols), df[y_cols]\n",
    "\n",
    "# Scale the data.\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_df)\n",
    "\n",
    "# Split the data into train/validation and test sets.\n",
    "X_y = [X_train_full, X_test, y_train_full, y_test] = train_test_split(\n",
    "    X_scaled, y_df, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "# Convert the data to tensors.\n",
    "X_y = [tf.convert_to_tensor(d) for d in X_y]\n",
    "[X_train_full, X_test, y_train_full, y_test] = X_y\n",
    "\n",
    "# Create a separate univariate data set.\n",
    "uv_cols = np.array([\n",
    "    bool(match(r'TOTALDEMAND|TM\\d+', e)) for e in X_df.columns\n",
    "])\n",
    "X_train_uv = tf.convert_to_tensor(X_train_full.numpy()[:,uv_cols])\n",
    "X_test_uv = tf.convert_to_tensor(X_test.numpy()[:,uv_cols])\n",
    "\n",
    "# Generate the time-series cross-validation train and validation sets. \n",
    "train_val_I = TimeSeriesSplit(n_splits=5).split(X_train_full)\n",
    "X_train_sets, y_train_sets, X_val_sets, y_val_sets = [], [], [], []\n",
    "\n",
    "for train, val in train_val_I:\n",
    "    # Print the traind and validation set lengths.\n",
    "    print(train.shape, val.shape) \n",
    "    val_start, val_end = val.min(), val.max()+1\n",
    "    X_train_sets.append(X_train_full[:val_start])\n",
    "    y_train_sets.append(y_train_full[:val_start])\n",
    "    X_val_sets.append(X_train_full[val_start: val_end])\n",
    "    y_val_sets.append(y_train_full[val_start: val_end])\n",
    "\n",
    "X_train_sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c2aed-3bf3-4469-a6fb-d851fa7c4c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e136ffaf-2a18-44d3-8653-59feed98243d",
   "metadata": {},
   "source": [
    "# Hyperparemeter tuning\n",
    "\n",
    "The code below is illustrative of how the gridsearch results were found. The hyperparameter values printed under each cell became the defaults in the `get_MLP` and `get_LSTM` functions after this experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "857c5aa2-7b5a-4dba-9107-397ccd3e6101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'opt_params': {'weight_decay': 1e-05, 'beta_1': 0.95},\n",
       " 'params': {'batch_size': 256}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# param_grid={\n",
    "#     'batch_size': [2**8, 2**9, 2**10],\n",
    "#     'optimizer__weight_decay': [1e-3, 1e-4, 1e-5],\n",
    "#     'optimizer__beta_1': [.8, .9, .95],\n",
    "# }\n",
    "\n",
    "# results = GridSearchCV(get_MLP, param_grid, epochs=300)\n",
    "\n",
    "# with open('Results/MLP gridsearch results.json', 'w') as f:\n",
    "#     f.write(dumps(results, indent=4))\n",
    "\n",
    "with open('Results/MLP gridsearch results.json', 'r') as f:\n",
    "    results = loads(f.read())\n",
    "    \n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e66ba380-1571-48ec-a571-c41c3e4b4be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'opt_params': {'weight_decay': 1e-05, 'beta_1': 0.8},\n",
       " 'params': {'batch_size': 256}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results = GridSearchCV(get_LSTM, param_grid, epochs=10)\n",
    "\n",
    "# with open('Results/BD-LSTM gridsearch results.json', 'w') as f:\n",
    "#     f.write(dumps(results, indent=4))\n",
    "\n",
    "with open('Results/BD-LSTM gridsearch results.json', 'r') as f:\n",
    "    results = loads(f.read())\n",
    "    \n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039a89d7-8b82-4643-a866-48a12ce0aec1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1f214df-0265-43ca-aed6-fad02ec7bf89",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "The code below is also somewhat illustrative. As the models were trained in iterations and asynchronously, the script that generated them never existed in one file. As such, I have provided the code to recreate or load the models we ultimately generated. The numbers of epochs listed are the total epochs we used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d44d3fa-3cce-4a96-bed8-d183c0c8db87",
   "metadata": {},
   "source": [
    "## Shallow multivariate MLP (M1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9011481b-1c90-491b-bf68-c9dea1c286fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# m1, m1_hist = train_model(\n",
    "#     y_train_full, X_test, y_test, \n",
    "#     epochs=10_000, **get_MLP(X_train_full, model_prefix='Final'),\n",
    "# )\n",
    "\n",
    "# m1.evaluate(X_test, y_test)\n",
    "\n",
    "# Load the model generated by the code above. \n",
    "m1 = load_model(\n",
    "    f\"models/{get_latest_models('MLP_shallow_multiv')}.keras\", \n",
    "    safe_mode=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95be87c6-d50d-4f73-bd4f-9b77cf491ba0",
   "metadata": {},
   "source": [
    "## Deep multivariate MLP (M2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff880d6-0feb-4d42-9637-c9f4f3dd4433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m2, m2_hist = train_model(\n",
    "#     y_train_full, X_test, y_test, \n",
    "#     epochs=4000, **get_MLP(X_train_full, 10, model_prefix='Final'),\n",
    "# )\n",
    "\n",
    "# m2.evaluate(X_test, y_test)\n",
    "\n",
    "# Load the model generated by the code above. \n",
    "m2 = load_model(\n",
    "    f\"models/{get_latest_models('MLP_deep_multiv')}.keras\", \n",
    "    safe_mode=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441ecb18-21df-4418-b75c-a9db7b266dbd",
   "metadata": {},
   "source": [
    "## Shallow multivariate BD-LSTM (M3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d32d00-b276-4ec2-9451-b857689a8f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m3, m3_hist = train_model(\n",
    "#     y_train_full, X_test, y_test, \n",
    "#     epochs=511, **get_LSTM(X_train_full, model_prefix='Final'),\n",
    "# )\n",
    "\n",
    "# m3.evaluate(X_test, y_test)\n",
    "\n",
    "# Load the model generated by the code above. \n",
    "m3 = load_lstm(get_latest_models('LSTM_shallow_multiv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09b98b0-9ff3-4cb7-8be5-a28ddd86d962",
   "metadata": {},
   "source": [
    "## Deep multivariate BD-LSTM (M4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01c91304-0fd6-46c6-a8a0-34ed1494eef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m4, m4_hist = train_model(\n",
    "#     y_train_full, X_test, y_test, \n",
    "#     epochs=189, **get_LSTM(X_train_full, 2, model_prefix='Final'),\n",
    "# )\n",
    "\n",
    "# m4.evaluate(X_test, y_test)\n",
    "\n",
    "# Load the model generated by the code above. \n",
    "m4 = load_lstm(get_latest_models('LSTM_deep_multiv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88634761-4997-4b0e-9604-e8423bfe0d76",
   "metadata": {},
   "source": [
    "## Shallow univariate MLP (M5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "aa5492ed-2f76-4347-bf63-2c785f5609b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m5, m5_hist = train_model(\n",
    "#    y_train_full, X_test_uv, y_test, epochs=10_500, \n",
    "#    **get_MLP(X_train_uv, univariate=True, model_prefix='Final'),\n",
    "# )\n",
    "\n",
    "# m5.evaluate(X_test_uv, y_test)\n",
    "\n",
    "# Load the model generated by the code above. \n",
    "m5 = load_model(\n",
    "    f\"models/{get_latest_models('MLP_shallow_univ')}.keras\", \n",
    "    safe_mode=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceb5767-f8e0-448b-803f-93e2731433f4",
   "metadata": {},
   "source": [
    "## Deep univariate MLP (M6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9c796f8-44d7-4519-987c-382d3b090bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m6, m6_hist = train_model(\n",
    "#     y_train_full, X_test_uv, y_test, epochs=3950, \n",
    "#     **get_MLP(X_train_uv, 10, univariate=True, model_prefix='Final'),\n",
    "# )\n",
    "\n",
    "# m6.evaluate(X_test_uv, y_test)\n",
    "\n",
    "# Load the model generated by the code above. \n",
    "m6 = load_model(\n",
    "    f\"models/{get_latest_models('MLP_deep_univ')}.keras\", \n",
    "    safe_mode=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d3a1de-3467-4507-ae1a-d91fab93ea76",
   "metadata": {},
   "source": [
    "## Shallow univariate BD-LSTM (M7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01012439-016a-41ee-9320-2c04eead401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m7, m7_hist = train_model(\n",
    "#     y_train_full, X_test_uv, y_test, epochs=315, \n",
    "#     **get_LSTM(X_train_uv, univariate=True, model_prefix='Final'),\n",
    "# )\n",
    "\n",
    "# m7.evaluate(X_test_uv, y_test)\n",
    "\n",
    "# Load the model generated by the code above. \n",
    "m7 = load_lstm(get_latest_models('LSTM_shallow_univ'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0b88e6-af99-433f-a2a7-338eba1df561",
   "metadata": {},
   "source": [
    "## Deep univariate BD-LSTM (M8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "987cc8b3-fe2a-4139-aece-a2a5df845e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m8, m8_hist = train_model(\n",
    "#     y_train_full, X_test_uv, y_test, epochs=150, \n",
    "#     **get_LSTM(X_train_uv, 2, univariate=True, model_prefix='Final'),\n",
    "# )\n",
    "\n",
    "# m8.evaluate(X_test_uv, y_test)\n",
    "\n",
    "# Load the model generated by the code above. \n",
    "m8 = load_lstm(get_latest_models('LSTM_deep_univ')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76d08c3-da5d-450b-b593-327aa27c2ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1afdeccb-1990-45bf-9820-01b1ad710743",
   "metadata": {},
   "source": [
    "# Model results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "620c1ad7-9792-43f4-bce4-4270f908117e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP_shallow_multiv</td>\n",
       "      <td>&lt;Functional name=Final_MLP_shallow_multiv, bui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LSTM_shallow_multiv</td>\n",
       "      <td>&lt;Functional name=Extended_Extended_Final_LSTM_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LSTM_shallow_univ</td>\n",
       "      <td>&lt;Functional name=Extended_Final_LSTM_shallow_u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LSTM_deep_multiv</td>\n",
       "      <td>&lt;Functional name=Extended_Extended_Final_LSTM_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLP_deep_multiv</td>\n",
       "      <td>&lt;Functional name=Final_MLP_deep_multiv, built=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LSTM_deep_univ</td>\n",
       "      <td>&lt;Functional name=Extended_Final_LSTM_deep_univ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLP_shallow_univ</td>\n",
       "      <td>&lt;Functional name=Final_MLP_shallow_univ, built...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLP_deep_univ</td>\n",
       "      <td>&lt;Functional name=Final_MLP_deep_univ, built=True&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model                                             object\n",
       "0   MLP_shallow_multiv  <Functional name=Final_MLP_shallow_multiv, bui...\n",
       "1  LSTM_shallow_multiv  <Functional name=Extended_Extended_Final_LSTM_...\n",
       "2    LSTM_shallow_univ  <Functional name=Extended_Final_LSTM_shallow_u...\n",
       "3     LSTM_deep_multiv  <Functional name=Extended_Extended_Final_LSTM_...\n",
       "4      MLP_deep_multiv  <Functional name=Final_MLP_deep_multiv, built=...\n",
       "5       LSTM_deep_univ  <Functional name=Extended_Final_LSTM_deep_univ...\n",
       "6     MLP_shallow_univ  <Functional name=Final_MLP_shallow_univ, built...\n",
       "7        MLP_deep_univ  <Functional name=Final_MLP_deep_univ, built=True>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the latest version of each model and store it in a dataframe.\n",
    "model_files = get_latest_models()\n",
    "models = dict()\n",
    "\n",
    "\n",
    "for m, n, f in model_files.values:\n",
    "    model = load_lstm(n) if 'LSTM' in m else load_model(f)\n",
    "    models[m] = model\n",
    "        \n",
    "models_df = pd.DataFrame(models.items(), columns=['model', 'object'])\n",
    "\n",
    "models_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "548a976f-e788-4d87-b662-0f83e2494c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4911/4911\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 148us/step - loss: 4.5896 - mean_squared_error: 110105.2500\n",
      "\u001b[1m4911/4911\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 6.2300 - mean_squared_error: 229217.9844\n",
      "\u001b[1m4911/4911\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 6.0058 - mean_squared_error: 242343.8125\n",
      "\u001b[1m4911/4911\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 10ms/step - loss: 5.5444 - mean_squared_error: 239986.5938\n",
      "\u001b[1m4911/4911\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 182us/step - loss: 5.5199 - mean_squared_error: 163765.6250\n",
      "\u001b[1m4911/4911\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 7ms/step - loss: 6.9838 - mean_squared_error: 472648.8750\n",
      "\u001b[1m4911/4911\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 144us/step - loss: 6.0822 - mean_squared_error: 172376.6719\n",
      "\u001b[1m4911/4911\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 186us/step - loss: 7.9082 - mean_squared_error: 230104.8281\n",
      "\u001b[1m4911/4911\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 166us/step\n",
      "\u001b[1m4911/4911\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step\n",
      "\u001b[1m4911/4911\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step\n",
      "\u001b[1m4911/4911\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 10ms/step\n",
      "\u001b[1m4911/4911\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 205us/step\n",
      "\u001b[1m4911/4911\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 17ms/step\n",
      "\u001b[1m4911/4911\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 166us/step\n",
      "\u001b[1m4911/4911\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 206us/step\n"
     ]
    }
   ],
   "source": [
    "# List the requisite X data for evaluating the models.\n",
    "models_df['X_test'] = models_df.model\\\n",
    "    .apply(lambda e: 'X_test_uv' if 'univ' in e else 'X_test')\n",
    "\n",
    "# Calculate the test MAPE and MSE.\n",
    "models_df[['test_MAPE', 'test_MSE']] = models_df.apply(lambda d: eval(\n",
    "    f\"d['object'].evaluate({d['X_test']}, y_test)\"\n",
    "), axis=1).apply(pd.Series)\n",
    "\n",
    "# Convert MSE to RMSE.\n",
    "models_df['test_RMSE'] = models_df.test_MSE**.5\n",
    "\n",
    "# Create the predictions for each model.\n",
    "models_df['y_pred_24H'] = models_df.apply(lambda d: eval(\n",
    "    f\"d['object'].predict({d['X_test']})\"\n",
    "), axis=1).apply(lambda d: [d[:,i].reshape(1, -1)[0] for i in range(2)])\n",
    "\n",
    "models_df['y_pred_1H'] = models_df.y_pred_24H.str.get(0)\n",
    "models_df['y_pred_24H'] = models_df.y_pred_24H.str.get(1)\n",
    "\n",
    "# Use the predictions to record the MAPE and RMSE \n",
    "# for each prediction horizon (i.e. 1 or 24 hours).\n",
    "for i, h in enumerate([1, 24]):\n",
    "    y_test_H = y_test[:,i]\n",
    "    models_df[f'test_MAPE_{h}H'] = models_df[f'y_pred_{h}H']\\\n",
    "        .apply(lambda d: float(MAPE(y_test_H, d)))\n",
    "    models_df[f'test_RMSE_{h}H'] = models_df[f'y_pred_{h}H']\\\n",
    "        .apply(lambda d: float(MSE(y_test_H, d)**.5))\n",
    "\n",
    "# Save the data to be reviewed in the results.\n",
    "models_df[[\n",
    "    'model', 'test_MAPE', 'test_RMSE', 'test_MAPE_1H', \n",
    "    'test_RMSE_1H', 'test_MAPE_24H', 'test_RMSE_24H'\n",
    "]].to_csv('Results/model test results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89955024-85bf-4386-9c58-044e8d4e3b47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
